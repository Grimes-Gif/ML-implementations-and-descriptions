{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Density estimation "
      ],
      "metadata": {
        "id": "Ab_05aKD_x8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K means clustering\n",
        "\n",
        "KMeans is trying to solve the following optimization problem:\n",
        "\n",
        "\\begin{align}\n",
        "\\arg \\min_S \\sum_{i=1}^K \\sum_{x_j \\in S_i} ||x_j - \\mu_i||^2\n",
        "\\end{align}\n",
        "where one needs to partition the N observations into K clusters: $S = \\{S_1, S_2, \\ldots, S_K\\}$ and each cluster has $\\mu_i$ as its center."
      ],
      "metadata": {
        "id": "RC_wJ22azkpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KMeans(object):\n",
        "    \n",
        "    def __init__(self): \n",
        "        pass\n",
        "    \n",
        "    def pairwise_dist(self, x, y):\n",
        "        np.random.seed(1)\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: N x D numpy array\n",
        "            y: M x D numpy array\n",
        "        Return:\n",
        "                dist: N x M array, where dist2[i, j] is the euclidean distance between \n",
        "                x[i, :] and y[j, :]\n",
        "        \"\"\"\n",
        "\n",
        "        return np.linalg.norm(x[:, None, :] - y[None, :, :], axis=-1)\n",
        "\n",
        "    def _init_centers(self, points, K, **kwargs):\n",
        "        np.random.seed(1)\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
        "            K: number of clusters\n",
        "            kwargs: any additional arguments you want\n",
        "        Return:\n",
        "            centers: K x D numpy array, the centers. \n",
        "        \"\"\"\n",
        "\n",
        "        return points[np.random.choice(points.shape[0], size=K, replace=False)]\n",
        "\n",
        "    def _update_assignment(self, centers, points):\n",
        "        np.random.seed(1)\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            centers: KxD numpy array, where K is the number of clusters, and D is the dimension\n",
        "            points: NxD numpy array, the observations\n",
        "        Return:\n",
        "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
        "        \"\"\"\n",
        "\n",
        "        distanceMatrix = self.pairwise_dist(centers, points)\n",
        "        return np.argmin(distanceMatrix, axis=0)\n",
        "   \n",
        "    \n",
        "    def _update_centers(self, old_centers, cluster_idx, points):\n",
        "        np.random.seed(1)\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            old_centers: old centers KxD numpy array, where K is the number of clusters, and D is the dimension\n",
        "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
        "            points: NxD numpy array, the observations\n",
        "        Return:\n",
        "            centers: new centers, K x D numpy array, where K is the number of clusters, and D is the dimension.\n",
        "        \"\"\"\n",
        "\n",
        "        #Need to querey all points that share the same cluster_idx\n",
        "        #Start by getting indecies of all points with cluster k\n",
        "        #access array with indecies\n",
        "\n",
        "        K = old_centers.shape[0]\n",
        "        for i in range(K):\n",
        "          cluster_point_indecies = np.where(cluster_idx == i) #get indecies of points in cluster i\n",
        "          cluster_points = points[cluster_point_indecies] #get points in cluster i\n",
        "          old_centers[i] = np.mean(cluster_points, axis=0) #assign the mean point to the old cluster\n",
        "\n",
        "        return old_centers\n",
        "\n",
        "    def _get_loss(self, centers, cluster_idx, points):\n",
        "        np.random.seed(1)\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            centers: KxD numpy array, where K is the number of clusters, and D is the dimension\n",
        "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
        "            points: NxD numpy array, the observations\n",
        "        Return:\n",
        "            loss: a single float number, which is the objective function of KMeans. \n",
        "        \"\"\"\n",
        "\n",
        "        #step 1: l2 norm of the cluster points subtracted from their centroids\n",
        "        #step 2: sum over the points\n",
        "        #step 3: repeat fore each cluster\n",
        "\n",
        "        K = centers.shape[0]\n",
        "        sum = 0.0\n",
        "        for i in range(K):\n",
        "          cluster_point_indecies = np.where(cluster_idx == i) #get indecies of points in cluster i\n",
        "          cluster_point = points[cluster_point_indecies] #get points in cluster i\n",
        "          inner_operation = (np.linalg.norm(cluster_point - centers[i], axis=1, ord=2))**2 #apply l2 norm\n",
        "          sum += np.sum(inner_operation)\n",
        "        return sum\n",
        "        \n",
        "    def __call__(self, points, K, max_iters=100, abs_tol=1e-16, rel_tol=1e-16, verbose=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
        "            K: number of clusters\n",
        "            max_iters: maximum number of iterations (Hint: You could change it when debugging)\n",
        "            abs_tol: convergence criteria w.r.t absolute change of loss\n",
        "            rel_tol: convergence criteria w.r.t relative change of loss\n",
        "            verbose: boolean to set whether method should print loss (Hint: helpful for debugging)\n",
        "            kwargs: any additional arguments you want\n",
        "        Return:\n",
        "            cluster assignments: Nx1 int numpy array\n",
        "            cluster centers: K x D numpy array, the centers\n",
        "            loss: final loss value of the objective function of KMeans\n",
        "        \"\"\"\n",
        "\n",
        "        centers = self._init_centers(points, K, **kwargs)\n",
        "        for it in range(max_iters):\n",
        "            cluster_idx = self._update_assignment(centers, points)\n",
        "            centers = self._update_centers(centers, cluster_idx, points)\n",
        "            loss = self._get_loss(centers, cluster_idx, points)\n",
        "            K = centers.shape[0]\n",
        "            if it:\n",
        "                diff = np.abs(prev_loss - loss)\n",
        "                if diff < abs_tol and diff / prev_loss < rel_tol:\n",
        "                    break\n",
        "            prev_loss = loss\n",
        "            if verbose:\n",
        "                print('iter %d, loss: %.4f' % (it, loss))\n",
        "        return cluster_idx, centers, loss\n",
        "    \n",
        "    def find_optimal_num_clusters(self, data, max_K=15,iteration = 100):\n",
        "        np.random.seed(1)\n",
        "        \"\"\"\n",
        "        Plots loss values for different number of clusters in K-Means\n",
        "        \n",
        "        Args:\n",
        "            data: input data of shape(samples, 2)\n",
        "            max_K: number of clusters\n",
        "        Return:\n",
        "            losses: a list, which includes the loss values for different number of clusters in K-Means\n",
        "            Plot loss values against number of clusters\n",
        "        \"\"\"\n",
        "\n",
        "        plot_data = []\n",
        "        for i in range(max_K):\n",
        "          results = self.__call__(data, i + 1)\n",
        "          plot_data.append(results[2]) \n",
        "\n",
        "        plt.scatter(np.arange(1, 16), plot_data)\n",
        "        return plot_data"
      ],
      "metadata": {
        "id": "HMY-Z5WX_VnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian mixture model\n",
        "\n",
        "A Gaussian Mixure Model (GMM) is a probabilistic model that assumes all the data points are generated from a mixure of a finite number of Gaussian Distributions. In a nutshell, GMM is a soft clustering algorithm in a sense that each data point is assigned to a cluster with a probability. In order to do that, we need to convert our clustering problem into an inference problem.\n",
        "\n",
        "Given $N$ samples $\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N]^T$, where $\\mathbf{x}_i \\in \\mathbb{R}^D$. Let $\\mathbf{\\pi}$ be a $K$-dimensional probability distribution and $(\\mathbf{\\mu}_k; \\mathbf{\\Sigma}_k)$ be the mean and covariance matrix of the $k^{th}$ Gaussian distribution in $\\mathbb{R}^D$. \n",
        "\n",
        "The GMM object implements an EM algorithm for fitting the model and MLE for optimizing its parameters. It also has certain hypothesis on how the data was generated:\n",
        "\n",
        "- Each data point $\\mathbf{x}_i$ is assigned to a cluster $k$ with probability of $\\pi_k$ where $\\sum_{k=1}^K \\pi_k = 1$\n",
        "- Each data point $\\mathbf{x}_i$ is generated from a multivariate normal distribution $\\cal{N}(\\mu_k, \\Sigma_k)$ where $\\mu_k \\in \\mathbb{R}^D$ and $\\Sigma_k \\in \\mathbb{R}^{D\\times D}$\n",
        "\n",
        "Our Goal is to find $K$-dimension gaussian distributions to model our data $\\mathbf{X}$. This can be done by learning the parameters $\\mathbf{\\pi}, \\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$. The detailed derivation can be found in our slide of GMM. The log-likelihood function now becomes:\n",
        "\n",
        "\\begin{align}\n",
        "    \\text{ln } p(\\mathbf{x}_1, \\dots, \\mathbf{x}_N | \\mathbf{\\pi}, \\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\sum_{i=1}^N \\text{ln } \\big( \\sum_{k=1}^{K} \\pi(k) \\mathcal{N}(\\mathbf{x}_i | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\\big)\n",
        "\\end{align}\n",
        "\n",
        "- **E-step:** Evaluate the responsibilities\n",
        "\n",
        "In this step, we need to calculate the responsibility $\\gamma$, which is the conditional probability that a datapoint belongs to a specific cluster $k$ if we are given the datapoint, i.e. $P(z_k|x)$. The formula for $\\tau$ is given below:\n",
        "\n",
        "$$\n",
        "\\gamma\\left(z_k\\right)=\\frac{\\pi_{k} N\\left(\\mathbf{x} | \\mathbf{\\mu}_{k}, \\mathbf{\\Sigma}_{k}\\right)}{\\sum_{j=1}^{K} \\pi_{j} N\\left(\\mathbf{x} | \\mathbf{\\mu}_{j}, \\mathbf{\\Sigma}_{j}\\right)}, \\quad \\text{for } k = 1, \\dots, K\n",
        "$$\n",
        "Note that each data point should have one probability for each component/cluster. For this homework, you will work with $\\gamma\\left(z_k\\right)$ which has a size of $N\\times K$ and you should have all the responsibility values in one matrix.\n",
        "\n",
        "- **M-step:** Re-estimate parameters\n",
        "\n",
        "After we obtained the responsibility, we can find the update of parameters, which are given below:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{\\mu}_k^{new} &= \\dfrac{\\sum_{n=1}^N \\gamma(z_k)\\mathbf{x}_n}{N_k} \\\\\n",
        "\\mathbf{\\Sigma}_k^{new} &= \\dfrac{1}{N_k}\\sum_{n=1}^N \\gamma (z_k)^T(\\mathbf{x}_n - \\mu_k^{new})^T(\\mathbf{x}_n-\\mu_k^{new}) \\\\\n",
        "\\pi_k^{new} &= \\dfrac{N_k}{N}\n",
        "\\end{align}\n",
        "where $N_k = \\sum_{n=1}^N \\gamma(z_k)$. Note that the updated value for $\\mu_k$ is used when updating $\\mathbf{\\Sigma}_k$. The multiplication of $\\gamma (z_k)^T(\\mathbf{x}_n - \\mu_k^{new})^T$ is element-wise so it will preserve the dimensions of $(\\mathbf{x}_n - \\mu_k^{new})^T$.\n",
        "\n",
        "- We repeat E and M steps until the incremental improvement to the likelihood function is small."
      ],
      "metadata": {
        "id": "cpZdkbUQzqS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SIGMA_CONST = 1e-6\n",
        "LOG_CONST = 1e-32\n",
        "import math\n",
        "\n",
        "class GMM(object):\n",
        "    def __init__(self, X, K, max_iters = 100):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            X: the observations/datapoints, N x D numpy array\n",
        "            K: number of clusters/components\n",
        "            max_iters: maximum number of iterations (used in EM implementation)\n",
        "        \"\"\"\n",
        "\n",
        "        self.points = X\n",
        "        self.max_iters = max_iters\n",
        "        \n",
        "        self.N = self.points.shape[0]        #number of observations\n",
        "        self.D = self.points.shape[1]        #number of features\n",
        "        self.K = K                           #number of components/clusters\n",
        "\n",
        "    \n",
        "    def softmax(self, logit):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logit: N x D numpy array\n",
        "        Return:\n",
        "            prob: N x D numpy array. See the above function.\n",
        "        \"\"\"\n",
        "\n",
        "        logit -= (np.amax(logit, axis=1))[:, None] #numerically stable\n",
        "        X = np.exp(logit)\n",
        "        return X / (np.sum(X, axis=1))[:, None]\n",
        "\n",
        "\n",
        "\n",
        "    def logsumexp(self, logit):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logit: N x D numpy array\n",
        "        Return:\n",
        "            s: N x 1 array where s[i,0] = logsumexp(logit[i,:]). See the above function\n",
        "        \"\"\"\n",
        "\n",
        "        #numerically stable\n",
        "        logit -= (np.amax(logit, axis=1))[:, None]\n",
        "        return np.log(np.sum(np.exp(logit), axis=1)[:, None])\n",
        "\n",
        "\n",
        "    def multinormalPDF(self, logits, mu_i, sigma_i):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            logits: N x D numpy array\n",
        "            mu_i: 1xD numpy array, the center for the ith gaussian.\n",
        "            sigma_i: 1xDxD numpy array, the covariance matrix of the ith gaussian.  \n",
        "        Return:\n",
        "            normal_pdf: 1xN numpy array, the probability distribution of N data for the ith gaussian\n",
        "        \"\"\"\n",
        "\n",
        "        lowerExp = logits.shape[1] / 2\n",
        "        det = np.linalg.det(sigma_i)\n",
        "\n",
        "        #computer normalization constant\n",
        "        doublePi = 2 * np.pi\n",
        "        bottomLeft = doublePi**lowerExp\n",
        "        bottomRight = det**1/2\n",
        "        const = bottomLeft * bottomRight\n",
        "        normal_const = 1 / const\n",
        "\n",
        "        #compute the probabilities\n",
        "        inverse_sigma = np.linalg.inv(sigma_i)\n",
        "        X_mu = logits - mu_i\n",
        "        leftSide = np.dot(X_mu, inverse_sigma)\n",
        "        inner_exp = np.transpose(leftSide) * np.transpose(X_mu)\n",
        "        inner_exp *= -1/2\n",
        "        sum_exp = np.sum(inner_exp, axis=0)\n",
        "        rightSide = np.exp(sum_exp)\n",
        "        final = normal_const * rightSide\n",
        "\n",
        "        return final\n",
        "    \n",
        "    def _init_components(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            kwargs: any other arguments you want\n",
        "        Return:\n",
        "            pi: numpy array of length K, prior\n",
        "            mu: KxD numpy array, the center for each gaussian. \n",
        "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. \n",
        "                You will have KxDxD numpy array for full covariance matrix case\n",
        "        \"\"\"\n",
        "\n",
        "        pi = np.full(self.K, 1/self.K) \n",
        "        mu = self.points[np.random.choice(self.points.shape[0], size=self.K, replace=False)]\n",
        "        sigma = np.zeros(shape=(self.K, self.D, self.D))\n",
        "        for k in range(self.K):\n",
        "          np.fill_diagonal(sigma[k], np.random.rand(1)) \n",
        "\n",
        "        return (pi, mu, sigma)\n",
        "\n",
        "    \n",
        "    def _ll_joint(self, pi, mu, sigma, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pi: np array of length K, the prior of each component\n",
        "            mu: KxD numpy array, the center for each gaussian. \n",
        "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. You will have KxDxD numpy\n",
        "            array for full covariance matrix case\n",
        "            \n",
        "        Return:\n",
        "            ll(log-likelihood): NxK array, where ll(i, k) = log pi(k) + log NormalPDF(points_i | mu[k], sigma[k])\n",
        "        \"\"\"\n",
        "\n",
        "        ll = np.zeros(shape=(self.N, self.K))\n",
        "        for k in range(self.K): \n",
        "          lpdf = np.log(pi[k]) + np.log(self.multinormalPDF(self.points, mu[k], sigma[k]))\n",
        "          ll[:, k] = lpdf \n",
        "        return ll\n",
        "\n",
        "\n",
        "    def _E_step(self, pi, mu, sigma, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pi: np array of length K, the prior of each component\n",
        "            mu: KxD numpy array, the center for each gaussian. \n",
        "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.You will have KxDxD numpy\n",
        "            array for full covariance matrix case\n",
        "        Return:\n",
        "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
        "        \"\"\"\n",
        "\n",
        "        X = self._ll_joint(pi, mu, sigma)\n",
        "        return self.softmax(X)\n",
        "\n",
        "\n",
        "    def _M_step(self, gamma, **kwargs): \n",
        "        \"\"\"\n",
        "        Args:\n",
        "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
        "        Return:\n",
        "            pi: np array of length K, the prior of each component\n",
        "            mu: KxD numpy array, the center for each gaussian. \n",
        "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. You will have KxDxD numpy\n",
        "            array for full covariance matrix case\n",
        "        \"\"\"\n",
        "\n",
        "        class_gamma = np.sum(gamma, axis=0)\n",
        "        u_pi = np.zeros(shape=(self.K))\n",
        "        u_mu = np.zeros(shape=(self.K, self.D))\n",
        "        u_sigma = np.zeros(shape=(self.K, self.D, self.D))\n",
        "        for k in range(self.K):\n",
        "          u_pi[k] = class_gamma[k] / self.N\n",
        "          u_mu[k] = np.sum((gamma[:, k, None] * self.points), axis=0) / class_gamma[k] \n",
        "          X_mu = self.points - u_mu[k]\n",
        "          u_sigma[k] = (1 / class_gamma[k]) * (np.dot(np.transpose(gamma)[k] * np.transpose(X_mu), X_mu))\n",
        "        \n",
        "\n",
        "        return (u_pi, u_mu, u_sigma)\n",
        "    \n",
        "    \n",
        "    def __call__(self, abs_tol=1e-16, rel_tol=1e-16, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            abs_tol: convergence criteria w.r.t absolute change of loss\n",
        "            rel_tol: convergence criteria w.r.t relative change of loss\n",
        "            kwargs: any additional arguments you want\n",
        "        \n",
        "        Return:\n",
        "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
        "            (pi, mu, sigma): (1xK np array, KxD numpy array, KxDxD numpy array)       \n",
        "        \"\"\"\n",
        "\n",
        "        pi, mu, sigma = self._init_components(**kwargs)\n",
        "        pbar = tqdm(range(self.max_iters))\n",
        "        \n",
        "        for it in pbar:\n",
        "            # E-step\n",
        "            gamma = self._E_step(pi, mu, sigma)\n",
        "            \n",
        "            # M-step\n",
        "            pi, mu, sigma = self._M_step(gamma)\n",
        "            \n",
        "            # calculate the negative log-likelihood of observation\n",
        "            joint_ll = self._ll_joint(pi, mu, sigma)\n",
        "            loss = -np.sum(self.logsumexp(joint_ll))\n",
        "            if it:\n",
        "                diff = np.abs(prev_loss - loss)\n",
        "                if diff < abs_tol and diff / prev_loss < rel_tol:\n",
        "                    break\n",
        "            prev_loss = loss\n",
        "            pbar.set_description('iter %d, loss: %.4f' % (it, loss))\n",
        "        return gamma, (pi, mu, sigma)"
      ],
      "metadata": {
        "id": "Qs1Ep9g4ztOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality reduction\n",
        "\n",
        "\n",
        "## PCA\n",
        "\n",
        "Assume a dataset is composed of N datapoints, each of which has D features with D < N.  The *dimension* of our data would be $D$. It is possible, however, that many of these dimensions contain redundant information.  The *intrinsic dimensionality* is the number of dimensions we need to reconstruct our data with high fidelity.  For our purposes, we will define the intrinsic dimension as the number of principal components needed to reconstruct 99% of the variation within our data.\n",
        "\n",
        "We define a set of features as linearly independent if we cannot construct one out of a linear combination of the others. The number of linearly independent features is the number of nonzero principal components (where we define 0 is anything less than $10^{-11}$ due to floating point error). Zero principal components mean that we can not find any weights to linearly combine features in order to create an indenpendent feature. Thus, our algorithm will assign 0 to these weights.\n"
      ],
      "metadata": {
        "id": "kHN37kadEZu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Dimensionality(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def pca(self, X):\n",
        "        \"\"\"\n",
        "        Decompose dataset into principal components using singular value decomposition. \n",
        "\n",
        "        Step 1: Calculate the mean (mu) and covariance matrix (C) from data\n",
        "        Step 2: Find eigenvectors (W) and values (lambda) of C in descending order\n",
        "        Step 3: Compute reduced feature space -> W[(X - mu) / sqrt(lambda)] for every point X(n)\n",
        "\n",
        "        Idea: Maximize the variance along a new dimension with the constrain that it is the unit vector\n",
        "        - use lagrangian optimization\n",
        "        - Take the derivative and set it to zero to find optima\n",
        "        - we find that the new dimensions that maximize the function are the ones that are the eigenvectors of the covariance matrix\n",
        "\n",
        "        Args: \n",
        "            X: N x D array corresponding to a dataset, in which N is the number of points and D is the number of features\n",
        "        Return:\n",
        "            U: N x N \n",
        "            S: min(N, D) elements\n",
        "            V: D x D\n",
        "        \"\"\"\n",
        "        return np.linalg.svd(X)\n",
        "\n",
        "    def recovered_variance_proportion(self, S, k):\n",
        "        \"\"\" \n",
        "        Compute the proportion of the variance in the original matrix recovered by a rank-k approximation\n",
        "\n",
        "        Args:\n",
        "            S: min(N, D) elements, 1-D array\n",
        "            k: scalar int, rank of approximation\n",
        "        Return:\n",
        "            recovered_var: float corresponding to proportion of recovered variance\n",
        "        \"\"\"\n",
        "\n",
        "        singular_values = S[:k]\n",
        "        return np.sum(singular_values**2) / np.sum(S**2)\n",
        "\n",
        "\n",
        "\n",
        "    def intrinsic_dimension(self, S, recovered_variance=.99):\n",
        "        \"\"\"\n",
        "        Find the number of principal components necessary to recover given proportion of variance\n",
        "\n",
        "        Args: \n",
        "            S: 1-d array corresponding to the singular values of a dataset\n",
        "\n",
        "            recovered_varaiance: float in [0,1].  Minimum amount of variance \n",
        "                to recover from given principal components\n",
        "        Return:\n",
        "            dim: int, the number of principal components necessary to recover \n",
        "                the given proportion of the variance\n",
        "        \"\"\"\n",
        "        for i in range(S.shape[0]):\n",
        "          rec_var = self.recovered_variance_proportion(S, i)\n",
        "          if (rec_var >= recovered_variance):\n",
        "            return i\n",
        "\n",
        "\n",
        "    def num_linearly_ind_features(self, S, eps=1e-11):\n",
        "        \"\"\"\n",
        "        Find the number of linearly independent features in dataset\n",
        "\n",
        "        Args: \n",
        "            S: 1-d array corresponding to the singular values of a dataset\n",
        "        Return:\n",
        "            dim: int, the number of linearly independent dimensions in our data\n",
        "        \"\"\"\n",
        "        count = 0\n",
        "        for i in range(S.shape[0]):\n",
        "          if S[i]**2 > eps:\n",
        "            count += 1\n",
        "\n",
        "        return count\n",
        "\n",
        "\n",
        "    def apply_PCA(self, X):\n",
        "        \"\"\"\n",
        "        Apply the functions you just implemented\n",
        "        Args: \n",
        "            X: N x D array corresponding to a dataset, in which N is the number of points and D is the number of features\n",
        "            retained variance: floating number\n",
        "        Return:\n",
        "            (X_new, num_linearly_ind_features, intrinsic_dimension): The X projection on the new feature space, the number of linearly independent dimensions in our data, the intrinsic dimension\n",
        "        \"\"\"\n",
        "        U, S, V = self.pca(X)\n",
        "        numLIF = self.num_linearly_ind_features(S)\n",
        "        ID = self.intrinsic_dimension(S)\n",
        "        return (np.matmul(X, np.transpose(V)), numLIF, ID)"
      ],
      "metadata": {
        "id": "LWtwUOMQEfRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#linear regression:\n",
        "\n",
        "Linear regression is the first supervised learning algorithm we'll look at. When it comes to supervision learning, we have two broad categories, classification, and regression.\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "**Remember, the purpose of the machine learning workflow is to find**:\n",
        "\n",
        "$f(X) ≈ Y$\n",
        "\n",
        "*when given some input X in* $ℝ^{NxD}$ *and some labeled output Y in* $ℝ^N$\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "Classification occurs when the values of our output $Y$ are discrete and regression occurs when the values are continuous. These are useful because large datasets tend to have a very high amount of variance making it virtually impossible for any search based or brute force solution viable, for example hand written numbers. Having an algorithm to accompodate for the variance in handwriting was one of the first applications of ML, the model being used for sorting mail.\n",
        "\n",
        "\\\\\n",
        "### key point\n",
        "Supervised learning models undergo two phases when being produced: \n",
        "- training: part of the dataset should be used so that the algorithm can begin to fit its model to the data and learn the parameters.\n",
        "- testing: the rest of the dataset should be used so that the algorithm can test whether or not this fit is accurate.\n",
        "\n",
        "\\\\\n",
        "### Using a basis for regression\n",
        "\n",
        "Assume $Y$ is a linear function of $X$\n",
        "\n",
        "$y(X, W) = w_0 +  ... + w_Dx_D$\n",
        "\n",
        "where $X$ is the input vector and $W$ is a weight vector containing a bias term $w_0$\n",
        "\n",
        "\\\\\n",
        "\n",
        "We can extend this notation using a basis function $Φ_m(X)$ since a linear relationship may not always be the case and we would like to be able to control the degree of the polynomial we want to fit:\n",
        "\n",
        "$y(X, W) = w_0 + ∑^{M-1}_{m=0}w_mΦ_m(x)$\n",
        "\n",
        "The basis function phi, takes a vector in $ℝ^D$ and gives a vector in $ℝ^M$. This allows us to capture nonlinearities within in our linear regression, as the weights that we are learning in the new dimensional space are still linear.\n",
        "\n",
        "\\\\\n",
        "\n",
        "We can further simplify this expression if we assign $Φ_0(x) = 1$. The expression becomes: $W^TΦ(X)$ \n",
        "\n",
        "Where $W$ is the weight vector and phi is the design matrix: the input X with the applied basis function. This reformating is true since the bias term $w_0$ will remain unweighted in the inner product.\n",
        "\n",
        "All this sounds well, but we have one problem ... we don't know what the weights are suppose to be!\n",
        "\n",
        "\\\\\n",
        "## learning weights for linear regression\n",
        "\n",
        "We assume that the target variable of our output $Y$, we can call it $t$ for target, is given by the sum of the deterministic function $y(X, W)$ and some random noise $ϵ$ for good measure:\n",
        "\n",
        "$t = y(X, W) + ϵ$\n",
        "\n",
        "Our objective now is to find the weight vector $W$ that will minimize the difference between the real values and predicted values on the model (but also making sure that its not zero, we'll talk about that later)\n",
        "\n",
        "\\\\\n",
        "\n",
        "Given N datapoints, we can find $W$ that will minimize the sum-of-squares using the least squares method. Our objective function is thus:\n",
        "\n",
        "$L(W) = \\frac{1}{2}∑_{n=1}^N(t_n - W^TΦ(x_n))^2$\n",
        "\n",
        "This formula simply takes the sum of the squared differences between the target values and the predicted values. To find the optimal W, as with most optimization solutions, we take the derivative w/ respect to $W$ and set it equal to zero and solve.\n",
        "\n",
        "$\\frac{ΔL(W)}{ΔW} = ∑^N_{n=1}(t_n - W^TΦ(x_n))Φ(x_n)^T$\n",
        "\n",
        "Setting the result equal to zero and expanding some terms we get the following:\n",
        "\n",
        "$w = (Φ^TΦ)^{-1}Φ^Tt$\n",
        "\n",
        "Where phi is the design matrix as mentioned earlier (applying the basis function to every point to the input $X$)\n",
        "\n",
        "This changes the dimensions of your dataset from $N x D$ to $N x M$ where $M$ is the degree of your basis function.\n",
        "\n",
        "\\\\\n",
        "We have a closed form solution to our optimization which is good! But we still have a big problem, and that is we are doing an inversion operation in it. This is a problem because invertable matricies must be singular (linearly independent) which is a very strict requirement considering real world data is very messy. It also should be mentioned that inverting a matrix has a horrible time complexity $O(n^3)$, and should be avoided in most cases.\n",
        "\n",
        "We have an open solution however that is much better however:\n",
        "\n",
        "\\\\\n",
        "## Gradient descent\n",
        "\n",
        "Going back to our objective function (adding 1/N instead of 1/2 for normilization), we know that the negative of the gradient of our function\n",
        "\n",
        "$\\frac{ΔL(W)}{ΔW} = \\frac{1}{N}∑^N_{n=1}(t_n - W^TΦ(x_n))Φ(x_n)^T$\n",
        "\n",
        "will always point in the direction of greatest descent. This means we can iterativly follow the derivative of our objective function to find local minima. The problem is that its a local minima, not a global minima, so getting stuck with subpar weights is possible without certain methods to overcome this.\n",
        "\n",
        "\\\\\n",
        "### Batch gradient descent\n",
        "- We sum the gradients over the entire dataset and do it all in one go. We add in a hyperparameter alpha, called the learning rate. This tells us how much we should follow the gradient per iteration. Steps too large may cause us to overshoot and oscillate over the local optima, steps that are too small will take too long to converge.\n",
        "\n",
        "$W_{τ+1} = W_τ - α\\frac{ΔL(W)}{ΔW}$\n",
        "\n",
        "pros: fast-converging, easy to implement\n",
        "cons: need to read all our data...\n",
        "\n",
        "\\\\\n",
        "### Stochastic gradient descent\n",
        "- randomly picks a point and follows the gradient till it hits zero. The learning rate is called beta in this case.\n",
        "\n",
        "$W_{τ+1} = W_τ - β\\frac{ΔL(W)}{ΔW}$\n",
        "\n",
        "However in this case it is without the sum, only one data point is considered at a time:\n",
        "\n",
        "$\\frac{ΔL(W)}{ΔW} = (t_n - W^TΦ(x_n))Φ(x_n)^T$\n",
        "\n",
        "\\\\\n",
        "### The problem of overfitting\n",
        "\n",
        "We know we can fit varying degrees of polynomial functions to our data, but overfitting can be a very big problem. If our data captures too much of the variance in our data, accounting for outliers and everything, its likely that our model will not make very good predictions for data outside its training dataset as it weights its training data far too heavily.\n",
        "\n",
        "**We then know that the problem of overfitting stems from the fact that each feature is being weighed too much in our training dataset, or in other words we are capturing the variance of one particular dataset too much, and losing its generalizability.**\n",
        "- Naturally, we want a solution that tells our regression to stop fitting over the data when the weights achieve a certain value.\n",
        "\n",
        "\\\\\n",
        "## Regularization\n",
        "\n",
        "Another way to word this solution, is that we want optomize our regression to give the minimum error, given that we stay within a certain range of values. For anyone who has taken a multi class, we understand that this means we must perform constrained optomization!\n",
        "\n",
        "Let $E_D(W)$ be our sum of squares error we covered earlier, a function that takes in our weights and finds the error with respect to the data. Taking a page from lagrangian multipliers, we know if we want to optomize this function with a constraint, we must add the constriant function times some factor of lambda. To formalize our constraint we should say that the magnitude of the weight vector shouldnt exceed a certain length, formally this would be through using a norm. \n",
        "\n",
        "Thus (The l2 norm squared is the same as taking the inner product):\n",
        "\n",
        "$W^TW ≤ C$\n",
        "\n",
        "Where C is some threshold\n",
        "\n",
        "\n",
        "We know from lagrangian multipliers, that the gradient where the function and constraint are tangential are proportional, meaning one is simply a multiple of the other and visversa, we can call this $λ$\n",
        "\n",
        "Finding the derivative of both the constraint and error function we get the following expression:\n",
        "\n",
        "\\\\\n",
        "Thus we get:\n",
        "\n",
        "$E_T(W) = E_D(W) + λE_W(W)$\n",
        "\n",
        "where\n",
        "\n",
        "$E_W(W) = ∑^M_{m=1}|w_m|^q$\n",
        "\n",
        "where q is a hyperparameter that we must choose for the type of optomization. Note that if q is equal to two, than it can be written as $W^TW$\n",
        "\n",
        "## Ridge regression\n",
        "\n",
        "Put all together for the case of q = 2 (called ridge regression), we get the following expression:\n",
        "\n",
        "$E_T(W) = \\frac{1}{2}(t - ΦW)^T(t-ΦW) + \\frac{λ}{2}W^TW$\n",
        "\n",
        "minimizing this functions derivative will give us the new values of the weights that account for overfitting. Whats more is that doing so results in a closed form solution! \n",
        "\n",
        "$\\frac{δE(W)}{δW} = (Φ^TΦ + λI)^{-1}Φ^Tt $\n",
        "\n",
        "Where lambda is our choice for how strict we want to regularize our data, $I$ is the identity matrix\n",
        "\n",
        "Now you've probably already seen it, but we have another inverse operation!\n",
        "\n",
        "## Lasso regression\n",
        "\n",
        "Another technique for regularization is to have our regulation term be q = 1, instead of 2. This gives us the sum expression you saw earlier: \n",
        "\n",
        "$∑^{M-1}_{m=0}|w_m| ≤ C$\n",
        "\n",
        "\\\\\n",
        "\n",
        "### Choosing lambda\n",
        "\n",
        "We have introduced some new hyperparameters lambda and q, but how do we know what choice is best? For things like q, its not so hard since the choices are limited due to its discrete nature 1, 2 ... etc., but lambda is completely arbitrary and continious.\n",
        "\n",
        "In order to choose we must do some testing, one way is to do leave-one-out cross-validation. For increasing values of lambda, train your model on a portion of the training data, except the nth data point. Rinse and repeat switching the nth data point to the next till you've done it N times. The series of test errors on the N left out points, called the validation set, is averaged and graphed. Doing this process for varying lambda will give you an empircal way to choose the best lambda, the best value being the minimum validation error.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\\\\n",
        "# Naive bayes classifier:\n",
        "\n",
        "$p(t=i|X) = \\frac{p(X|t = i)p(t = i)}{p(X)}$\n",
        "\n",
        "Given the likelihood of our data, the prior and evidence we can calculate the posterior using bayes rule.\n",
        "\n",
        "What makes it naive is that we asusme that the label and features are conditionally independent. Problem is the naive assumption may be bad, and we dont explicitly calculate any boundary\n",
        "\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\\\\n",
        "# Logistic regression:\n",
        "\n",
        "Define some terms:\n",
        "\n",
        "$S = W^TΦ(X)$\n",
        "\n",
        "\n",
        "$g(s) = \\frac{e^s}{1 + e^s}$\n",
        "\n",
        "$h(x) = p(t|X) = g(s)$\n",
        "\n",
        "\\\\\n",
        "model:\n",
        "\n",
        "$p(t|X) = g(s)^{t_n}(1-g(s))^{1-t_n}$\n",
        "\n",
        "\n",
        "\\\\\n",
        "Take MLE of $p(t|X)$\n",
        "\n",
        "$ll(W) = log ∏^N_{n=1}(p(t|X))$\n",
        "\n",
        "When we take derivative with respect to weights however, we dont get a closed form solution.\n",
        "\n",
        "The solution is to use the gradient, and since the negative of the log likeli hood function is concave, we know there is one global optima. We take the negative of the gradient to go down for a convex function and the gradient itself if its concave.\n",
        "\n",
        "Gradient:\n",
        "\n",
        "$\\frac{Δll(W)}{ΔW} = ∑^N_{n=1}(Φ(X_n)(t_n- 1) + (1 - g(s))Φ(X_n)$\n",
        "\n",
        "GD algo:\n",
        "\n",
        "$W_{τ+1} = W_τ - α\\frac{Δll(W)}{ΔW}$\n",
        "\n",
        "$While ||W_{τ+1} - W_τ|| > ϵ$\n",
        "\n",
        "Where epsilon is the threshold to stop\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Z_bKWXxvtku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression practice"
      ],
      "metadata": {
        "id": "CoQ2yYeaKXHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1KpQo3RvktB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "08fdf009-dc26-4f83-ecde-3c2c84d90aa2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7d1bdc8fd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#import some libraries to create a feature space\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#create some arbitrary dataset with 2 features\n",
        "data = np.linspace(2.0, 20.0, 10)\n",
        "#create some noisy targets from your dataset\n",
        "targets = np.log(data) + np.random.normal(0, .3, 10)\n",
        "\n",
        "#visualize data\n",
        "plt.scatter(data, targets, color='green')\n",
        "\n",
        "#draw out trendline\n",
        "plt.plot(np.linspace(2, 20, 80), np.log(np.linspace(2, 20, 80)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First start by coding the objective function, which is the sum of the squares formula"
      ],
      "metadata": {
        "id": "4W1k1RhdLr0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the objective function that we want to optimize\n",
        "def objective_func(weights, targets, design_M):\n",
        "  return 1/2 * np.sum((targets - np.dot(weights, design_M))**2)\n",
        "\n",
        "#create a design matrix using simple basis function\n",
        "def create_DM(data, degree):\n",
        "  \"\"\"\n",
        "    take data, an NxD matrix and give a Nx(Degree) design matrix\n",
        "  \"\"\"\n",
        "  data = np.transpose(np.vstack([data**i for i in range(degree)]))\n",
        "  return data\n",
        "\n",
        "# M = 2\n",
        "design_matrix = create_DM(data, 2)\n",
        "\n",
        "# Get data dependent error\n",
        "objective_func()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIIQ8sGaLq_N",
        "outputId": "2ce5c5e7-91cc-4bb7-a3e9-f5f18c98c02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  2.]\n",
            " [ 1.  4.]\n",
            " [ 1.  6.]\n",
            " [ 1.  8.]\n",
            " [ 1. 10.]\n",
            " [ 1. 12.]\n",
            " [ 1. 14.]\n",
            " [ 1. 16.]\n",
            " [ 1. 18.]\n",
            " [ 1. 20.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive bayes: email spam not spam classifier\n"
      ],
      "metadata": {
        "id": "RR7hUJRL1P9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "class Naive_Bayes(object):\n",
        "\n",
        "    def __init__(self): \n",
        "        # load emails\n",
        "        x = open('./emails.txt').read()\n",
        "        emails = json.loads(x)\n",
        "        \n",
        "        # get previous spam emails (spam), non spam emails (not_spam), unclassified input mails (to_classify)\n",
        "        spam = emails[\"spam\"]\n",
        "        not_spam = emails[\"not_spam\"]\n",
        "        to_classify = emails[\"to_classify\"]\n",
        "        \n",
        "        # Number of emails\n",
        "        n_spam = len(spam)\n",
        "        n_not_spam = len(not_spam)\n",
        "        n_to_classify = len(to_classify)\n",
        "        \n",
        "        ''' To ignore certain common words in English that might skew your model, we add them to the stop words \n",
        "         list below. You may want to experiment by choosing your own list of stop words, \n",
        "         but be sure to keep subject in this list at a minimum, as it appears in every email content.'''\n",
        "        stop_words = text.ENGLISH_STOP_WORDS.union({'subject'})        \n",
        "        \n",
        "        # Form bag of words model using words used at least 10 times\n",
        "        vectorizer = text.CountVectorizer(stop_words=stop_words,min_df=10)\n",
        "        X = vectorizer.fit_transform(spam+not_spam+to_classify).toarray()\n",
        "        \n",
        "        # split word counts into separate matrices\n",
        "        self.X_spam, self.X_not_spam, self.X_to_classify = X[:n_spam,:], X[n_spam:n_spam+n_not_spam,:], X[n_spam+n_not_spam:,:]\n",
        "\n",
        "\n",
        "    def _likelihood_ratio(self, X_spam, X_not_spam):\n",
        "        '''\n",
        "        Args:\n",
        "            X_spam: n_spam x d where n_spam is the number of spam emails,\n",
        "                and d is the number of unique words that were there in all the emails\n",
        "            X_not_spam: n_not_spam x d where n_not_spam is the number of good emails,\n",
        "                and d is the number of unique words that were there in all the emails\n",
        "        Return:\n",
        "            ratio: 1 x d vector of the likelihood ratio of different words (spam/not_spam)\n",
        "        '''\n",
        "  \n",
        "        #get the number of times each word appears in spam\n",
        "        spam_total_appearances = np.sum(X_spam, axis=0)\n",
        "        spam_total_appearances += 1\n",
        "        likelihood_of_spam = spam_total_appearances / np.sum(spam_total_appearances)\n",
        "\n",
        "        #get the number times each word appears in non_spam\n",
        "        nonSpam_total_appearances = np.sum(X_not_spam, axis=0)\n",
        "        nonSpam_total_appearances += 1\n",
        "        likelihood_of_nonSpam = nonSpam_total_appearances / np.sum(nonSpam_total_appearances)\n",
        "\n",
        "        return likelihood_of_spam / likelihood_of_nonSpam\n",
        "\n",
        "\n",
        "    def _priors_ratio(self, X_spam, X_not_spam):\n",
        "        '''\n",
        "        Args:\n",
        "            X_spam: n_spam x d where n_spam is the number of spam emails,\n",
        "                and d is the number of unique words that were there in all the emails\n",
        "            X_not_spam: n_not_spam x d where n_not_spam is the number of good emails,\n",
        "                and d is the number of unique words that were there in all the emails\n",
        "        Return:\n",
        "            pr: prior ratio of (spam/not_spam)\n",
        "        '''\n",
        "        num_Words = X_spam.shape[1]\n",
        "        X_spam_prior = np.sum(X_spam) / num_Words \n",
        "        X_nonSpam_prior = np.sum(X_not_spam) / num_Words\n",
        "\n",
        "        return X_spam_prior / X_nonSpam_prior\n",
        "\n",
        "    def classify_spam(self, likelihood_ratio, pratio, X_to_classify):\n",
        "        '''\n",
        "        Args:\n",
        "            likelihood_ratio: 1 x d vector of ratio of likelihoods of different words\n",
        "            pratio: a scalar\n",
        "            X_to_classify: bag of words representation of the unknown emails. k x d, where \n",
        "                k is the number of emails to classify and d is the number of unique words that were there in all the emails\n",
        "        Return:\n",
        "             resolved: 1 x k list, each entry is 'S' to indicate spam or 'NS' to indicate not spam. \n",
        "             k is the number of emails to classify\n",
        "        '''\n",
        "        resolved = []\n",
        "        for i in range(X_to_classify.shape[0]):\n",
        "          email_wordCount = X_to_classify[i]\n",
        "          guess = np.prod(likelihood_ratio**(email_wordCount)) * pratio\n",
        "          if guess > 1:\n",
        "            resolved.append('S')\n",
        "          else:\n",
        "            resolved.append('NS') \n",
        "\n",
        "        return resolved\n",
        "\n"
      ],
      "metadata": {
        "id": "1qqAjMnp1PHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision trees\n",
        "\n",
        "Sometimes finding a boundary of your dataset is not a singular calculation. Multiple types of boundary at varying locations may be needed.\n",
        "\n",
        "Each boundary drawn on the data is referred to as a split.\n",
        "\n",
        "Decision trees classify data using if-then logic:\n",
        "- Each split is called a fork, and they split into two branches based on some value.\n",
        "- The goal is to maximize the information gain out of each split\n",
        "- You keep splitting once all your leaf nodes are pure, meaning they have only 1 kind of labeled data\n",
        "\n",
        "We can use the entropy gain from the split to determine if any of the D features are a good split for iteration i.\n",
        "\n",
        "$H(X) = - ∑^K_{k=1} p(x = k) log_2p(x=k)$\n",
        "\n",
        "High entropy → all the classes are likely\n",
        "Low entropy → a few classes are likely\n",
        "\n",
        "In this case entropy captures the purity of our nodes, if the objective is maximize the information gain per split, then naturally the numerical goal here is to find the split that gives the lowest entropy, as entropy is a measure of ambiguity of our data.\n",
        "\n",
        "For consecutive splits the conditional entropy can be used \n",
        "- Given that we split on: $H_{before}$\n",
        "- The average conditional entropy after splitting would be: $H_{after} = prob_LH_L + prob_RH_R$\n",
        "- Maximize information gain: $IG = H - H_{after}$\n",
        "\n",
        "For discrete feature spaces formatting our tree is a trivial task, however for a continuous feature space, this may not be so simple. One possible solution is to discretize your feature space but you may end up calculating the entropy gain for splits on empty space. To avoid this, it is easier to consider each split on each data point in the data set, that way no split is trivial."
      ],
      "metadata": {
        "id": "gynckFMD7NBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LearnTree(X, t):\n",
        "  '''\n",
        "  inputs: \n",
        "    X: set of N training vectors containing D features\n",
        "    t: vector of N elements where t[n] = class of X[n] \n",
        "  '''"
      ],
      "metadata": {
        "id": "t71i2VkzF2pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble learning\n",
        "\n",
        "Decision trees while useful have some issues.\n",
        "- Sometimes computing the information gain is not trivial\n",
        "- The tree can get extremely large and possibly overfit the training dataset.\n",
        "\n",
        "What can we do?\n",
        "- Aquire more training data: this is a bit of a non-solution as this is frequently impossible, or not practicle\n",
        "- Grow full tree, then post prune\n",
        "- ensemble learning\n",
        "\n",
        "## Reduced error pruning\n",
        "\n",
        "Split data into training and validation sets\n",
        "\n",
        "Grow tree based on training set\n",
        "- 1. Evaluate impact on validation set of pruning each possible node\n",
        "- 2. Greedily remove the node that most imrpoves validation set accuracy\n",
        "\n",
        "\n",
        "## Random forests\n",
        "\n",
        "Create a 'jury' of several decision trees that will weigh a new decision on a given point\n",
        "\n",
        "Common strategies: Bagging (bootstrap aggregating)\n",
        "\n"
      ],
      "metadata": {
        "id": "V1YfRdPHHIBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RandomForest(object):\n",
        "    def __init__(self, n_estimators=4, max_depth=4, feat_sample_rate=0.1):\n",
        "        self.n_estimators = n_estimators    # number of trees in the forest\n",
        "        self.max_depth = max_depth          # maximum depth a tree can take\n",
        "        self.feat_sample_rate = feat_sample_rate    # feature sub sample rate\n",
        "        self.bootstraps_row_indices = []    # subsampled row indices\n",
        "        self.feature_indices = []           # indices of subsampled features\n",
        "        self.out_of_bag = []\n",
        "        self.decision_trees = [sklearn.tree.DecisionTreeClassifier(max_depth=max_depth, criterion='entropy') for i in range(n_estimators)]  # list containing sklearn decision trees objects\n",
        "        \n",
        "\n",
        "\n",
        "    #Return hyperparameters set at instantiation\n",
        "    def getParams(self):\n",
        "      return (self.n_estimators, self.max_depth, self.feat_sample_rate)\n",
        "\n",
        "\n",
        "\n",
        "    def _bootstrapping(self, num_training, num_features):\n",
        "        \"\"\"\n",
        "        TODO: \n",
        "        - Randomly select a sample dataset of size num_training **with** replacement from the original dataset. \n",
        "        - Randomly select certain number of features (num_features denotes the total number of features in X, \n",
        "          feat_sample_rate denotes the percentage of features that are used to fit each decision tree) **without** replacement from the total number of features.\n",
        "        \n",
        "        Return:\n",
        "        - row_idx: the row indices corresponding to the row locations of the selected samples in the original dataset.\n",
        "        - col_idx: the column indices corresponding to the column locations of the selected features in the original feature list.\n",
        "        \"\"\"\n",
        "        \n",
        "        percent = int(self.feat_sample_rate * num_features)\n",
        "\n",
        "        rows = np.random.choice(num_training, num_training)\n",
        "        cols = np.random.choice(num_features, percent, replace=False)\n",
        "        return rows, cols\n",
        "         \n",
        "    def bootstrapping(self, num_training, num_features):\n",
        "        # Initializing the bootstap datasets for each tree\n",
        "        for i in range(self.n_estimators):\n",
        "            total = set(list(range(num_training)))\n",
        "            row_idx, col_idx = self._bootstrapping(num_training, num_features)\n",
        "            total = total - set(row_idx)\n",
        "            self.bootstraps_row_indices.append(row_idx)\n",
        "            self.feature_indices.append(col_idx)\n",
        "            self.out_of_bag.append(total)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "        Train decision trees using the bootstrapped datasets.\n",
        "        Note that you need to use the row indices and column indices.\n",
        "        \n",
        "        X: NxD numpy array, where N is number \n",
        "           of instances and D is the dimensionality of each \n",
        "           instance\n",
        "        y: Nx1 numpy array, the corresponding target labels\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "        self.bootstrapping(N, D)\n",
        "\n",
        "        for i, tree in enumerate(self.decision_trees):\n",
        "          data_points = self.bootstraps_row_indices[i]\n",
        "\n",
        "          bagged_points = X[data_points]\n",
        "          training_labels = y[data_points]\n",
        "\n",
        "          training_set = bagged_points[:, self.feature_indices[i]]\n",
        "\n",
        "          tree.fit(training_set, training_labels)\n",
        "\n",
        "        \n",
        "\n",
        "    def OOB_score(self, X, y):\n",
        "        # This function computes the accuracy of the random forest model predicting y given x.\n",
        "        accuracy = []\n",
        "        for i in range(len(X)):\n",
        "            predictions = []\n",
        "            for t in range(self.n_estimators):\n",
        "                if i in self.out_of_bag[t]:\n",
        "                    predictions.append(self.decision_trees[t].predict(np.reshape(X[i][self.feature_indices[t]], (1,-1)))[0])\n",
        "            if len(predictions) > 0:\n",
        "                accuracy.append(np.sum(predictions == y[i]) / float(len(predictions)))\n",
        "        return np.mean(accuracy)"
      ],
      "metadata": {
        "id": "99oAbsypC6qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support vector machines\n",
        "\n",
        "We want to build a classification system given a training set\n",
        "\n",
        "$(X_n, t_n)$ for $n = 1, ... N$\n",
        "\n",
        "Linearly seperable vs not_linearly seperable:\n",
        "- Draws the distinction on whether or not a linear function can appropiately group the data with 100 percent accuracy.\n",
        "\n",
        "\n",
        "Linear classifiers have the form:\n",
        "\n",
        "$f(x) = b + W^TX$\n",
        "\n",
        "- where b is the bias term, W is the vector of weights and X is the input data\n",
        "- In a linear function the weights are simply the norm to the line\n",
        "\n",
        "## Perceptron algorithm\n",
        "\n",
        "1. Initialize **W** = 0 (including bias term, b)\n",
        "2. Go though each datapoint $(x_n,t_n)$\n",
        "3. If $x_n$ is misclassified, then $W_{τ+1} ← W_τ + αt_nx_n$\n",
        "4. Until all datapoints are correctly classified\n",
        "\n",
        "\n",
        "Sounds good so far, but this only works if the data is linearly seperable. We are essentially just drawing a line through our data.\n",
        "\n",
        "That being said... What is the best line to draw? Multiple types of linear classifications may exist, the best one should be the one that maximizes the seperation between the classes of data.\n",
        "- Most stable under perturbations of the inputs (data)\n",
        "\n",
        "Its important to note that $W$ is perpendicular to the decision line\n",
        "\n",
        "$W^TX_j + b$= 0 and $W^TX_i + b = 0$\n",
        "\n",
        "$W^TX_j = W^TX$ therefor $W^T(X_j - X_i) = 0$\n",
        "\n",
        "### Finding the distance\n",
        "\n",
        "We can computer the distance between any point and the decision line by projecting $(X_n - X_i)$ onto **W**. **W** must be normal to obtain a unit vector\n",
        "\n",
        "*distance* = $\\frac{|w^T(x_n-x_i|}{||w||_2}$\n",
        "\n",
        "after simplification:\n",
        "\n",
        "*distance* = $\\frac{1}{||w||_2}$\n",
        "\n",
        "to find the margin we are considering the distance on both sides so we simply find 2 * *distance*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JUEiYcD8C6_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network\n",
        "\n",
        "In neural networks, a neuron corresponds to a unit that outputs: $$z_{i} = h \\left( \\sum \\limits_{j=1}^{n} w_{ij}x_{j}+b_{i} \\right) = h(\\mathbf{w}_{i}^{T}\\mathbf{x}+b_{i})$$ where $\\mathbf{w}_{i} \\in R^{D}$ is the weight vector, $\\mathbf{x} \\in R^{D}$ is ONE data point with $D$ features, $b_{i} \\in R$ is the bias element, and $h(.)$ is any non linear function that will be described later. \n",
        "\n",
        "### Fully-connected Layer\n",
        "Typically, a modern neural network contains thousands of neurons. Neurons interact in different configurations. In this part we describe a fully connected layer configuration in a neural network, which uses parallel neurons to form a layer.\n",
        "\n",
        "We extend the previous notation to describe a fully connected layer as follows\n",
        "$$\\mathbf{z} = h(\\mathbf{a}), \\quad \\mathbf{a}=\\mathbf{W}\\mathbf{x}+\\mathbf{b}$$ where $\\mathbf{a} \\in R^{M}$ is the output vector after appling linear operations, $\\mathbf{W} \\in R^{M \\times N}$ is the weight matrix, and $\\mathbf{b} \\in R^{M}$ is the bias vector. \n",
        "\n",
        "Therefore, we can use the neuron layer to update the data signal. The whole operation can be summarized as,\n",
        "$$\\mathbf{z}^{[l]} = h(\\mathbf{W}\\mathbf{z}^{[l-1]}+\\mathbf{b}) $$ where $\\mathbf{z}^{[l-1]}$ is the output of the previous layer as shown in figure below. Since we are going to build a two layer neural network model, $l \\in \\{1, 2\\}$ in this problem.\n",
        "\n",
        "\n",
        "### Activation Function\n",
        "There are many kinds of activation function. For this question, we are going to use Relu and Sigmoid.\n",
        "#### ReLU\n",
        "The Rectified Linear Unit (ReLU) is one of the most commonly used activation function in neural network models. The function is $h(a)=max(0,a)$. ReLU shares a lot of the properties of linear functions and it tends to work well on most of the problems. The only issue is that the derivative is not defined at a = 0, which we can overcome by assigning the derivative to 0 at a = 0. However, this means that for a ≤ 0 the gradient is zero and again can’t learn.\n",
        "#### Sigmoid\n",
        "The sigmoid function is another non-linear function with S-shaped curve. This function is useful in the case of binary classification as its output is between 0 and 1. The mathematical form of the function is $h(a)=\\frac{1}{1+e^{-a}}$. One of the main disadvantages for using the sigmoid function on hidden layers is that the gradient is very close to zero over a large portion of its domain which makes it slow and harder for the learning algorithm to learn.\n",
        "\n",
        "![Relu](https://drive.google.com/uc?id=10g4b7WTbt9Y9QQQ9EJhiqXhQvfEpiXLe)\n",
        "\n",
        "### Mean Squared Error\n",
        "It is an estimator that measures the average of the squares of the errors i.e. the average squared difference between the actual and the estimated values. It estimates the quality of the learnt hypothesis between the actual and the predicted values. It's non-negative and closer to zero, the better the learnt function is.\n",
        "\n",
        "#### Implementation details\n",
        "For regression problems as in this exercise, we compute the loss as follows:\n",
        "\n",
        "$$E = \\frac{1}{2N}\\sum\\limits_{i=1}^{N}\\left(y_{i} - \\hat{y_{i}}\\right)^{2}$$\n",
        "\n",
        "where $y_{i}$ is the true label and $\\hat{y_{i}}$ is the estimated label. We use a factor of $\\frac{1}{2N}$ instead of $\\frac{1}{N}$ to simply the derivative of loss function.\n",
        "\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "## Convolutional?\n",
        "\n",
        "Our neural net in its original state has many, many weights that we need to learn from our data, so we must ask the daunting question... can we simplify this? \n",
        "\n",
        "To accomplish this task we need to reduce our inputs, we can achieve this by running our inputs every so often through a filter, known as a convolutional layer in which we sum up all the items in the filter. This allows our neural net to consider more general concepts like instead of considering every piece of information individually.\n",
        "\n",
        "Consider an image classifier, instead of using every individual pixel as an imput, we might instead bundle pixels in groups that share similar features. If we wanted our neural net to identify a ball in pictures, the information of the ball might be lost amongst the dissection amongst every single pixel input, so instead we might learn the weights of the summed pixel values of that ball.\n",
        "\n",
        "This allows us to achieve something known as translational invariance, it allows our neural net to learn what data visualizations look like such as audio and image data. Instead of learning the pixel values of an image, we start learning shapes within the image it self. This way no matter the orientation or position of the shape in space, the weights will capture it.\n",
        "\n",
        "\n",
        "### Applying convolutions\n",
        "\n",
        "Maxpooling: take the maximum of each convolution to create an even smaller network\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-ooDsL6ayiVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from math import exp\n",
        "from random import random, randint, choice\n",
        "\n",
        "class Perceptron(object):\n",
        "    \"\"\"\n",
        "    Class to represent a single Perceptron in the net.\n",
        "    \"\"\"\n",
        "    def __init__(self, inSize=1, weights=None):\n",
        "        self.inSize = inSize+1 #number of perceptrons feeding into this one; add one for bias\n",
        "        if weights is None:\n",
        "            #weights of previous layers into this one, random if passed in as None\n",
        "            self.weights = [1.0]*self.inSize\n",
        "            self.setRandomWeights()\n",
        "        else:\n",
        "            self.weights = weights\n",
        "\n",
        "    def getWeightedSum(self, inActs):\n",
        "        \"\"\"\n",
        "        Returns the sum of the input weighted by the weights.\n",
        "\n",
        "        Inputs:\n",
        "            inActs (list<float/int>): input values, same as length as inSize\n",
        "        Returns:\n",
        "            float\n",
        "            The weighted sum\n",
        "        \"\"\"\n",
        "\n",
        "        return sum([inAct*inWt for inAct,inWt in zip(inActs,self.weights)])\n",
        "\n",
        "    def sigmoid(self, value):\n",
        "        \"\"\"\n",
        "        Return the value of a sigmoid function.\n",
        "\n",
        "        Args:\n",
        "            value (float): the value to get sigmoid for\n",
        "        Returns:\n",
        "            float\n",
        "            The output of the sigmoid function parametrized by\n",
        "            the value.\n",
        "        \"\"\"\n",
        "\n",
        "        return 1 / (1 + exp(value * -1))\n",
        "\n",
        "    def sigmoidActivation(self, inActs):\n",
        "        \"\"\"\n",
        "        Returns the activation value of this Perceptron with the given input.\n",
        "        Same as g(z) in book.\n",
        "        Remember to add 1 to the start of inActs for the bias input.\n",
        "\n",
        "        Inputs:\n",
        "            inActs (list<float/int>): input values, not including bias\n",
        "        Returns:\n",
        "            float\n",
        "            The value of the sigmoid of the weighted input\n",
        "        \"\"\"\n",
        "  \n",
        "        biasedList = self.addBiase(inActs)\n",
        "        return self.sigmoid(self.getWeightedSum(biasedList))\n",
        "\n",
        "    def sigmoidDeriv(self, value):\n",
        "        \"\"\"\n",
        "        Return the value of the derivative of a sigmoid function.\n",
        "\n",
        "        Args:\n",
        "            value (float): the value to get sigmoid for\n",
        "        Returns:\n",
        "            float\n",
        "            The output of the derivative of a sigmoid function\n",
        "            parametrized by the value.\n",
        "        \"\"\"\n",
        " \n",
        "        return self.sigmoid(value) * (1 - self.sigmoid(value))\n",
        "\n",
        "    def sigmoidActivationDeriv(self, inActs):\n",
        "        \"\"\"\n",
        "        Returns the derivative of the activation of this Perceptron with the\n",
        "        given input. Same as g'(z) in book (note that this is not rounded.\n",
        "        Remember to add 1 to the start of inActs for the bias input.\n",
        "\n",
        "        Inputs:\n",
        "            inActs (list<float/int>): input values, not including bias\n",
        "        Returns:\n",
        "            int\n",
        "            The derivative of the sigmoid of the weighted input\n",
        "        \"\"\"\n",
        "    \n",
        "        biasedList = self.addBiase(inActs)\n",
        "        return self.sigmoidDeriv(self.getWeightedSum(biasedList))\n",
        "\n",
        "    def updateWeights(self, inActs, alpha, delta):\n",
        "        \"\"\"\n",
        "        Updates the weights for this Perceptron given the input delta.\n",
        "        Remember to add 1 to the start of inActs for the bias input.\n",
        "\n",
        "        Inputs:\n",
        "            inActs (list<float/int>): input values, not including bias\n",
        "            alpha (float): The learning rate\n",
        "            delta (float): If this is an output, then g'(z)*error\n",
        "                           If this is a hidden unit, then the as defined-\n",
        "                           g'(z)*sum over weight*delta for the next layer\n",
        "        Returns:\n",
        "            float\n",
        "            Return the total modification of all the weights (sum of each abs(modification))\n",
        "        \"\"\"\n",
        "\n",
        "        totalModification = 0\n",
        "        biasedList = self.addBiase(inActs)\n",
        "        for i, weight in enumerate(self.weights):\n",
        "            difference = biasedList[i] * delta * alpha\n",
        "            totalModification += abs(difference)\n",
        "            self.weights[i] += difference\n",
        "        return totalModification\n",
        "\n",
        "    def setRandomWeights(self):\n",
        "        \"\"\"\n",
        "        Generates random input weights that vary from -1.0 to 1.0\n",
        "        \"\"\"\n",
        "        for i in range(self.inSize):\n",
        "            self.weights[i] = (random() + .0001) * (choice([-1,1]))\n",
        "\n",
        "    def addBiase(self, list):\n",
        "        biasedList = list.copy()\n",
        "        biasedList.insert(0, 1)\n",
        "        return biasedList\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\" toString \"\"\"\n",
        "        outStr = ''\n",
        "        outStr += 'Perceptron with %d inputs\\n'%self.inSize\n",
        "        outStr += 'Node input weights %s\\n'%str(self.weights)\n",
        "        return outStr\n",
        "\n",
        "\n",
        "class NeuralNet(object):\n",
        "    \"\"\"\n",
        "    Class to hold the net of perceptrons and implement functions for it.\n",
        "    \"\"\"\n",
        "    def __init__(self, layerSize):\n",
        "        \"\"\"\n",
        "        Initiates the NN with the given sizes.\n",
        "\n",
        "        Args:\n",
        "            layerSize (list<int>): the number of perceptrons in each layer\n",
        "        \"\"\"\n",
        "        self.layerSize = layerSize #Holds number of inputs and percepetrons in each layer\n",
        "        self.outputLayer = []\n",
        "        self.numHiddenLayers = len(layerSize)-2\n",
        "        self.hiddenLayers = [[] for x in range(self.numHiddenLayers)]\n",
        "        self.numLayers =  self.numHiddenLayers+1\n",
        "\n",
        "        #build hidden layer(s)\n",
        "        for h in range(self.numHiddenLayers):\n",
        "            for p in range(layerSize[h+1]):\n",
        "                percep = Perceptron(layerSize[h]) # num of perceps feeding into this one\n",
        "                self.hiddenLayers[h].append(percep)\n",
        "\n",
        "        #build output layer\n",
        "        for i in range(layerSize[-1]):\n",
        "            percep = Perceptron(layerSize[-2]) # num of perceps feeding into this one\n",
        "            self.outputLayer.append(percep)\n",
        "\n",
        "        #build layers list that holds all layers in order - use this structure\n",
        "        # to implement back propagation\n",
        "        self.layers = [self.hiddenLayers[h] for h in range(self.numHiddenLayers)] + [self.outputLayer]\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"toString\"\"\"\n",
        "        outStr = ''\n",
        "        outStr +='\\n'\n",
        "        for hiddenIndex in range(self.numHiddenLayers):\n",
        "            outStr += '\\nHidden Layer #%d'%hiddenIndex\n",
        "            for index in range(len(self.hiddenLayers[hiddenIndex])):\n",
        "                outStr += 'Percep #%d: %s'%(index,str(self.hiddenLayers[hiddenIndex][index]))\n",
        "            outStr +='\\n'\n",
        "        for i in range(len(self.outputLayer)):\n",
        "            outStr += 'Output Percep #%d:%s'%(i,str(self.outputLayer[i]))\n",
        "        return outStr\n",
        "\n",
        "    def feedForward(self, inActs):\n",
        "        \"\"\"\n",
        "        Propagate input vector forward to calculate outputs.\n",
        "\n",
        "        Args:\n",
        "            inActs (list<float>): the input to the NN (an example)\n",
        "        Returns:\n",
        "            list<list<float/int>>\n",
        "            A list of lists. The first list is the input list, and the others are\n",
        "            lists of the output values of all perceptrons in each layer.\n",
        "        \"\"\"\n",
        " \n",
        "        outputByLayer = []\n",
        "        outputByLayer.append(inActs)\n",
        "\n",
        "        for i in range(0, self.numHiddenLayers):\n",
        "            current = []\n",
        "            for j in range(0, self.layerSize[i + 1]):\n",
        "                output = self.hiddenLayers[i][j].sigmoidActivation(inActs)\n",
        "                current.append(output)\n",
        "            outputByLayer.append(current)\n",
        "            inActs = current.copy()\n",
        "\n",
        "        current = []\n",
        "        for i in range(0, len(self.outputLayer)):\n",
        "            output = self.outputLayer[i].sigmoidActivation(inActs)\n",
        "            current.append(output)\n",
        "\n",
        "        outputByLayer.append(current)\n",
        "        return outputByLayer\n",
        "\n",
        "    def backPropLearning(self, examples, alpha):\n",
        "        \"\"\"\n",
        "        Run a single iteration of backward propagation learning algorithm.\n",
        "        See the text and slides for pseudo code.\n",
        "\n",
        "        Args:\n",
        "            examples (list<tuple<list<float>,list<float>>>):\n",
        "              for each tuple first element is input(feature)\"vector\" (list)\n",
        "              second element is output \"vector\" (list)\n",
        "            alpha (float): the alpha to training with\n",
        "        Returns\n",
        "           tuple<float,float>\n",
        "\n",
        "           A tuple of averageError and averageWeightChange, to be used as stopping conditions.\n",
        "           averageError is the summed error^2/2 of all examples, divided by numExamples*numOutputs.\n",
        "           averageWeightChange is the summed absolute weight change of all perceptrons,\n",
        "           divided by the sum of their input sizes (the average weight change for a single perceptron).\n",
        "        \"\"\"\n",
        "\n",
        "        #keep track of output\n",
        "        averageError = 0\n",
        "        averageWeightChange = 0\n",
        "        numWeights = 0\n",
        "        counter = 0\n",
        "\n",
        "        for example in examples:#for each example\n",
        "            #keep track of deltas to use in weight change\n",
        "            deltas = []\n",
        "            #Neural net output list\n",
        "            allLayerOutput = self.feedForward(example[0])\n",
        "            lastLayerOutput = allLayerOutput[-1]\n",
        "            #Empty output layer delta list\n",
        "            outDelta = []\n",
        "            #iterate through all output layer neurons\n",
        "            for outputNum in range(len(example[1])):\n",
        "                gPrime = self.outputLayer[outputNum].sigmoidActivationDeriv(allLayerOutput[-2])\n",
        "                error = example[1][outputNum] - lastLayerOutput[outputNum]\n",
        "                delta = error * gPrime\n",
        "                averageError+=error*error/2\n",
        "                outDelta.append(delta)\n",
        "            deltas.append(outDelta)\n",
        "\n",
        "            \"\"\"\n",
        "            Backpropagate through all hidden layers, calculating and storing\n",
        "            the deltas for each perceptron layer.\n",
        "            \"\"\"\n",
        "            for layerNum in range(self.numHiddenLayers-1,-1,-1):\n",
        "                layer = self.layers[layerNum]\n",
        "                nextLayer = self.layers[layerNum+1]\n",
        "                hiddenDelta = []\n",
        "                #Iterate through all neurons in this layer\n",
        "                for neuronNum in range(len(layer)):\n",
        "                    gPrime = layer[neuronNum].sigmoidActivationDeriv(allLayerOutput[layerNum])\n",
        "                    sum = 0\n",
        "                    for i in range(0, len(nextLayer)):\n",
        "                        sum += nextLayer[i].weights[neuronNum+1] * deltas[0][i]\n",
        "                    delta = sum * gPrime\n",
        "                    hiddenDelta.append(delta)\n",
        "                deltas = [hiddenDelta]+deltas\n",
        "\n",
        "            \"\"\"\n",
        "            Having aggregated all deltas, update the weights of the\n",
        "            hidden and output layers accordingly.\n",
        "            \"\"\"\n",
        "            for numLayer in range(0,self.numLayers):\n",
        "                layer = self.layers[numLayer]\n",
        "                for numNeuron in range(len(layer)):\n",
        "                    weightMod = layer[numNeuron].updateWeights(allLayerOutput[numLayer], alpha, deltas[numLayer][numNeuron])\n",
        "                    averageWeightChange += weightMod\n",
        "                    numWeights += layer[numNeuron].inSize\n",
        "            #end for each example\n",
        "        #calculate final output\n",
        "        averageError /= (len(examples)*len(examples[0][1]))             #number of examples x length of output vector\n",
        "        averageWeightChange/=(numWeights)\n",
        "        return averageError, averageWeightChange\n",
        "\n",
        "\n",
        "def buildNeuralNet(examples, alpha=0.1, weightChangeThreshold = 0.00008,hiddenLayerList = [1], maxItr = sys.maxsize, startNNet = None):\n",
        "    \"\"\"\n",
        "    Train a neural net for the given input.\n",
        "\n",
        "    Args:\n",
        "        examples (tuple<list<tuple<list,list>>,\n",
        "                        list<tuple<list,list>>>): A tuple of training and test examples\n",
        "        alpha (float): the alpha to train with\n",
        "        weightChangeThreshold (float):           The threshold to stop training at\n",
        "        maxItr (int):                            Maximum number of iterations to run\n",
        "        hiddenLayerList (list<int>):             The list of numbers of Perceptrons\n",
        "                                                 for the hidden layer(s).\n",
        "        startNNet (NeuralNet):                   A NeuralNet to train, or none if a new NeuralNet\n",
        "                                                 can be trained from random weights.\n",
        "    Returns\n",
        "       tuple<NeuralNet,float>\n",
        "\n",
        "       A tuple of the trained Neural Network and the accuracy that it achieved\n",
        "       once the weight modification reached the threshold, or the iteration\n",
        "       exceeds the maximum iteration.\n",
        "    \"\"\"\n",
        "    examplesTrain,examplesTest = examples\n",
        "    numIn = len(examplesTrain[0][0])\n",
        "    numOut = len(examplesTest[0][1])\n",
        "    time = datetime.now().time()\n",
        "    if startNNet is not None:\n",
        "        hiddenLayerList = [len(layer) for layer in startNNet.hiddenLayers]\n",
        "    print (\"Starting training at time %s with %d inputs, %d outputs, %s hidden layers, size of training set %d, and size of test set %d\"\\\n",
        "                                                    %(str(time),numIn,numOut,str(hiddenLayerList),len(examplesTrain),len(examplesTest)))\n",
        "    layerList = [numIn]+hiddenLayerList+[numOut]\n",
        "    nnet = NeuralNet(layerList)\n",
        "    if startNNet is not None:\n",
        "        nnet =startNNet\n",
        "\n",
        "    iteration=0\n",
        "    trainError=0\n",
        "    weightMod= float('inf')\n",
        "\n",
        "    for i in range(0, maxItr):\n",
        "        if (weightChangeThreshold < weightMod):\n",
        "            trainError, weightMod = nnet.backPropLearning(examplesTrain, alpha)\n",
        "            iteration += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    time = datetime.now().time()\n",
        "    print ('Finished after %d iterations at time %s with training error %f and weight change %f'%(iteration,str(time),trainError,weightMod))\n",
        "\n",
        "    testError = 0\n",
        "    testCorrect = 0\n",
        "    testAccuracy=0\n",
        "\n",
        "    for example in examplesTest:\n",
        "        allLayerOutput = nnet.feedForward(example[0])\n",
        "        lastLayerOutput = allLayerOutput[-1]\n",
        "        roundOut(lastLayerOutput)\n",
        "        if lastLayerOutput == example[1]:\n",
        "            testCorrect += 1\n",
        "        else:\n",
        "            testError += 1\n",
        "\n",
        "    testAccuracy = testCorrect / len(examplesTest)\n",
        "\n",
        "    print('Feed Forward Test correctly classified %d, incorrectly classified %d, test accuracy %f\\n'%(testCorrect,testError,testAccuracy))\n",
        "    return (nnet, testAccuracy)\n",
        "\n",
        "\n",
        "def roundOut(list):\n",
        "    for i in range(0, len(list)):\n",
        "        list[i] = round(list[i])"
      ],
      "metadata": {
        "id": "_dt6xGy03EXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}